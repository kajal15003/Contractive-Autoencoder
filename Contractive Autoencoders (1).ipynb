{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contractive Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T05:55:55.747408Z",
     "start_time": "2020-04-04T05:55:54.983687Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported all libraries successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "# import pdb\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "print(\"Imported all libraries successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T05:55:55.752884Z",
     "start_time": "2020-04-04T05:55:55.749558Z"
    }
   },
   "outputs": [],
   "source": [
    "class Args:\n",
    "    batch_size=128\n",
    "    epochs=19\n",
    "    no_cuda='store_true'\n",
    "    seed=1\n",
    "    log_interval=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T05:55:55.837398Z",
     "start_time": "2020-04-04T05:55:55.754514Z"
    }
   },
   "outputs": [],
   "source": [
    "args=Args()\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T05:55:55.949660Z",
     "start_time": "2020-04-04T05:55:55.840291Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "kwargs = {'num_workers': 5, 'pin_memory': True} if args.cuda else {}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "\tdatasets.MNIST('data', train=True, download=True,\n",
    "\t\ttransform=transforms.ToTensor()),\n",
    "\tbatch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "lam = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T05:55:55.988495Z",
     "start_time": "2020-04-04T05:55:55.951422Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class CAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, 400, bias = False) # Encoder\n",
    "        self.fc2 = nn.Linear(400, 784, bias = False) # Decoder\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def encoder(self, x):\n",
    "        h1 = self.relu(self.fc1(x.view(-1, 784)))\n",
    "        return h1\n",
    "\n",
    "    def decoder(self,z):\n",
    "        h2 = self.sigmoid(self.fc2(z))\n",
    "        return h2\n",
    "\n",
    "    def forward(self, x):\n",
    "            h1 = self.encoder(x)\n",
    "            h2 = self.decoder(h1)\n",
    "            return h1, h2\n",
    "# Writing data in a grid to check the quality and progress\n",
    "    def samples_write(self, x, epoch):\n",
    "        _, samples = self.forward(x)\n",
    "        #pdb.set_trace()\n",
    "        samples = samples.data.cpu().numpy()[:16]\n",
    "        fig = plt.figure(figsize=(4, 4))\n",
    "        gs = gridspec.GridSpec(4, 4)\n",
    "        gs.update(wspace=0.05, hspace=0.05)\n",
    "        for i, sample in enumerate(samples):\n",
    "            ax = plt.subplot(gs[i])\n",
    "            plt.axis('off')\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])\n",
    "            ax.set_aspect('equal')\n",
    "            plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "        if not os.path.exists('out/'):\n",
    "            os.makedirs('out/')\n",
    "        plt.savefig('out/{}.png'.format(str(epoch).zfill(3)), bbox_inches='tight')\n",
    "        #self.c += 1\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T06:07:47.336561Z",
     "start_time": "2020-04-04T05:55:55.992356Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch: 0 [0/60000(0%)]\t Loss: 544.433228\n",
      "Train epoch: 0 [1280/60000(2%)]\t Loss: 529.155823\n",
      "Train epoch: 0 [2560/60000(4%)]\t Loss: 504.667969\n",
      "Train epoch: 0 [3840/60000(6%)]\t Loss: 465.286865\n",
      "Train epoch: 0 [5120/60000(9%)]\t Loss: 408.449371\n",
      "Train epoch: 0 [6400/60000(11%)]\t Loss: 355.693298\n",
      "Train epoch: 0 [7680/60000(13%)]\t Loss: 301.379791\n",
      "Train epoch: 0 [8960/60000(15%)]\t Loss: 273.609283\n",
      "Train epoch: 0 [10240/60000(17%)]\t Loss: 249.846985\n",
      "Train epoch: 0 [11520/60000(19%)]\t Loss: 238.212341\n",
      "Train epoch: 0 [12800/60000(21%)]\t Loss: 233.998734\n",
      "Train epoch: 0 [14080/60000(23%)]\t Loss: 222.588135\n",
      "Train epoch: 0 [15360/60000(26%)]\t Loss: 221.900711\n",
      "Train epoch: 0 [16640/60000(28%)]\t Loss: 224.861176\n",
      "Train epoch: 0 [17920/60000(30%)]\t Loss: 218.619095\n",
      "Train epoch: 0 [19200/60000(32%)]\t Loss: 216.559525\n",
      "Train epoch: 0 [20480/60000(34%)]\t Loss: 210.707458\n",
      "Train epoch: 0 [21760/60000(36%)]\t Loss: 208.366196\n",
      "Train epoch: 0 [23040/60000(38%)]\t Loss: 204.688156\n",
      "Train epoch: 0 [24320/60000(41%)]\t Loss: 197.381699\n",
      "Train epoch: 0 [25600/60000(43%)]\t Loss: 189.273453\n",
      "Train epoch: 0 [26880/60000(45%)]\t Loss: 195.315231\n",
      "Train epoch: 0 [28160/60000(47%)]\t Loss: 189.043304\n",
      "Train epoch: 0 [29440/60000(49%)]\t Loss: 189.939606\n",
      "Train epoch: 0 [30720/60000(51%)]\t Loss: 183.214081\n",
      "Train epoch: 0 [32000/60000(53%)]\t Loss: 188.928085\n",
      "Train epoch: 0 [33280/60000(55%)]\t Loss: 180.228119\n",
      "Train epoch: 0 [34560/60000(58%)]\t Loss: 177.048431\n",
      "Train epoch: 0 [35840/60000(60%)]\t Loss: 178.947784\n",
      "Train epoch: 0 [37120/60000(62%)]\t Loss: 173.573853\n",
      "Train epoch: 0 [38400/60000(64%)]\t Loss: 169.473099\n",
      "Train epoch: 0 [39680/60000(66%)]\t Loss: 175.905197\n",
      "Train epoch: 0 [40960/60000(68%)]\t Loss: 175.508636\n",
      "Train epoch: 0 [42240/60000(70%)]\t Loss: 169.774368\n",
      "Train epoch: 0 [43520/60000(72%)]\t Loss: 163.802261\n",
      "Train epoch: 0 [44800/60000(75%)]\t Loss: 163.225235\n",
      "Train epoch: 0 [46080/60000(77%)]\t Loss: 166.572983\n",
      "Train epoch: 0 [47360/60000(79%)]\t Loss: 161.669327\n",
      "Train epoch: 0 [48640/60000(81%)]\t Loss: 157.774658\n",
      "Train epoch: 0 [49920/60000(83%)]\t Loss: 155.904907\n",
      "Train epoch: 0 [51200/60000(85%)]\t Loss: 156.468414\n",
      "Train epoch: 0 [52480/60000(87%)]\t Loss: 159.921722\n",
      "Train epoch: 0 [53760/60000(90%)]\t Loss: 159.392548\n",
      "Train epoch: 0 [55040/60000(92%)]\t Loss: 159.495407\n",
      "Train epoch: 0 [56320/60000(94%)]\t Loss: 153.429932\n",
      "Train epoch: 0 [57600/60000(96%)]\t Loss: 154.812759\n",
      "Train epoch: 0 [58880/60000(98%)]\t Loss: 148.589294\n",
      "====> Epoch: 0 Average loss: 222.4947\n",
      "Train epoch: 1 [0/60000(0%)]\t Loss: 149.931168\n",
      "Train epoch: 1 [1280/60000(2%)]\t Loss: 150.548508\n",
      "Train epoch: 1 [2560/60000(4%)]\t Loss: 148.207413\n",
      "Train epoch: 1 [3840/60000(6%)]\t Loss: 146.342728\n",
      "Train epoch: 1 [5120/60000(9%)]\t Loss: 149.789398\n",
      "Train epoch: 1 [6400/60000(11%)]\t Loss: 145.690689\n",
      "Train epoch: 1 [7680/60000(13%)]\t Loss: 145.852768\n",
      "Train epoch: 1 [8960/60000(15%)]\t Loss: 139.463196\n",
      "Train epoch: 1 [10240/60000(17%)]\t Loss: 144.041992\n",
      "Train epoch: 1 [11520/60000(19%)]\t Loss: 142.221558\n",
      "Train epoch: 1 [12800/60000(21%)]\t Loss: 140.222397\n",
      "Train epoch: 1 [14080/60000(23%)]\t Loss: 143.096909\n",
      "Train epoch: 1 [15360/60000(26%)]\t Loss: 139.743744\n",
      "Train epoch: 1 [16640/60000(28%)]\t Loss: 138.318665\n",
      "Train epoch: 1 [17920/60000(30%)]\t Loss: 134.636215\n",
      "Train epoch: 1 [19200/60000(32%)]\t Loss: 137.455826\n",
      "Train epoch: 1 [20480/60000(34%)]\t Loss: 135.885788\n",
      "Train epoch: 1 [21760/60000(36%)]\t Loss: 133.403793\n",
      "Train epoch: 1 [23040/60000(38%)]\t Loss: 133.161514\n",
      "Train epoch: 1 [24320/60000(41%)]\t Loss: 130.638275\n",
      "Train epoch: 1 [25600/60000(43%)]\t Loss: 134.749283\n",
      "Train epoch: 1 [26880/60000(45%)]\t Loss: 131.593430\n",
      "Train epoch: 1 [28160/60000(47%)]\t Loss: 138.889420\n",
      "Train epoch: 1 [29440/60000(49%)]\t Loss: 126.845665\n",
      "Train epoch: 1 [30720/60000(51%)]\t Loss: 132.537140\n",
      "Train epoch: 1 [32000/60000(53%)]\t Loss: 131.333527\n",
      "Train epoch: 1 [33280/60000(55%)]\t Loss: 127.309563\n",
      "Train epoch: 1 [34560/60000(58%)]\t Loss: 126.754730\n",
      "Train epoch: 1 [35840/60000(60%)]\t Loss: 130.725800\n",
      "Train epoch: 1 [37120/60000(62%)]\t Loss: 126.449379\n",
      "Train epoch: 1 [38400/60000(64%)]\t Loss: 131.939377\n",
      "Train epoch: 1 [39680/60000(66%)]\t Loss: 124.385887\n",
      "Train epoch: 1 [40960/60000(68%)]\t Loss: 127.348610\n",
      "Train epoch: 1 [42240/60000(70%)]\t Loss: 127.842896\n",
      "Train epoch: 1 [43520/60000(72%)]\t Loss: 126.038338\n",
      "Train epoch: 1 [44800/60000(75%)]\t Loss: 128.810867\n",
      "Train epoch: 1 [46080/60000(77%)]\t Loss: 130.907379\n",
      "Train epoch: 1 [47360/60000(79%)]\t Loss: 121.156952\n",
      "Train epoch: 1 [48640/60000(81%)]\t Loss: 124.581039\n",
      "Train epoch: 1 [49920/60000(83%)]\t Loss: 125.296555\n",
      "Train epoch: 1 [51200/60000(85%)]\t Loss: 120.819550\n",
      "Train epoch: 1 [52480/60000(87%)]\t Loss: 119.767494\n",
      "Train epoch: 1 [53760/60000(90%)]\t Loss: 124.626732\n",
      "Train epoch: 1 [55040/60000(92%)]\t Loss: 122.587303\n",
      "Train epoch: 1 [56320/60000(94%)]\t Loss: 127.463722\n",
      "Train epoch: 1 [57600/60000(96%)]\t Loss: 116.424637\n",
      "Train epoch: 1 [58880/60000(98%)]\t Loss: 121.247322\n",
      "====> Epoch: 1 Average loss: 132.9442\n",
      "Train epoch: 2 [0/60000(0%)]\t Loss: 121.028435\n",
      "Train epoch: 2 [1280/60000(2%)]\t Loss: 121.524399\n",
      "Train epoch: 2 [2560/60000(4%)]\t Loss: 122.277130\n",
      "Train epoch: 2 [3840/60000(6%)]\t Loss: 116.367188\n",
      "Train epoch: 2 [5120/60000(9%)]\t Loss: 118.267677\n",
      "Train epoch: 2 [6400/60000(11%)]\t Loss: 120.466888\n",
      "Train epoch: 2 [7680/60000(13%)]\t Loss: 117.179237\n",
      "Train epoch: 2 [8960/60000(15%)]\t Loss: 117.439384\n",
      "Train epoch: 2 [10240/60000(17%)]\t Loss: 119.407921\n",
      "Train epoch: 2 [11520/60000(19%)]\t Loss: 115.072906\n",
      "Train epoch: 2 [12800/60000(21%)]\t Loss: 112.402367\n",
      "Train epoch: 2 [14080/60000(23%)]\t Loss: 116.193283\n",
      "Train epoch: 2 [15360/60000(26%)]\t Loss: 116.226311\n",
      "Train epoch: 2 [16640/60000(28%)]\t Loss: 114.207161\n",
      "Train epoch: 2 [17920/60000(30%)]\t Loss: 113.858643\n",
      "Train epoch: 2 [19200/60000(32%)]\t Loss: 109.624252\n",
      "Train epoch: 2 [20480/60000(34%)]\t Loss: 121.287483\n",
      "Train epoch: 2 [21760/60000(36%)]\t Loss: 111.041451\n",
      "Train epoch: 2 [23040/60000(38%)]\t Loss: 115.567314\n",
      "Train epoch: 2 [24320/60000(41%)]\t Loss: 111.615807\n",
      "Train epoch: 2 [25600/60000(43%)]\t Loss: 108.930527\n",
      "Train epoch: 2 [26880/60000(45%)]\t Loss: 114.509819\n",
      "Train epoch: 2 [28160/60000(47%)]\t Loss: 115.312401\n",
      "Train epoch: 2 [29440/60000(49%)]\t Loss: 110.295509\n",
      "Train epoch: 2 [30720/60000(51%)]\t Loss: 108.548935\n",
      "Train epoch: 2 [32000/60000(53%)]\t Loss: 111.680542\n",
      "Train epoch: 2 [33280/60000(55%)]\t Loss: 110.215210\n",
      "Train epoch: 2 [34560/60000(58%)]\t Loss: 112.161537\n",
      "Train epoch: 2 [35840/60000(60%)]\t Loss: 107.900330\n",
      "Train epoch: 2 [37120/60000(62%)]\t Loss: 113.352997\n",
      "Train epoch: 2 [38400/60000(64%)]\t Loss: 109.453491\n",
      "Train epoch: 2 [39680/60000(66%)]\t Loss: 106.922432\n",
      "Train epoch: 2 [40960/60000(68%)]\t Loss: 110.326195\n",
      "Train epoch: 2 [42240/60000(70%)]\t Loss: 108.031700\n",
      "Train epoch: 2 [43520/60000(72%)]\t Loss: 100.788185\n",
      "Train epoch: 2 [44800/60000(75%)]\t Loss: 104.400887\n",
      "Train epoch: 2 [46080/60000(77%)]\t Loss: 105.140221\n",
      "Train epoch: 2 [47360/60000(79%)]\t Loss: 101.432365\n",
      "Train epoch: 2 [48640/60000(81%)]\t Loss: 105.972763\n",
      "Train epoch: 2 [49920/60000(83%)]\t Loss: 106.887215\n",
      "Train epoch: 2 [51200/60000(85%)]\t Loss: 101.372520\n",
      "Train epoch: 2 [52480/60000(87%)]\t Loss: 106.587402\n",
      "Train epoch: 2 [53760/60000(90%)]\t Loss: 107.505676\n",
      "Train epoch: 2 [55040/60000(92%)]\t Loss: 108.312881\n",
      "Train epoch: 2 [56320/60000(94%)]\t Loss: 103.697060\n",
      "Train epoch: 2 [57600/60000(96%)]\t Loss: 98.932388\n",
      "Train epoch: 2 [58880/60000(98%)]\t Loss: 103.367996\n",
      "====> Epoch: 2 Average loss: 111.0267\n",
      "Train epoch: 3 [0/60000(0%)]\t Loss: 100.679520\n",
      "Train epoch: 3 [1280/60000(2%)]\t Loss: 102.182892\n",
      "Train epoch: 3 [2560/60000(4%)]\t Loss: 106.287933\n",
      "Train epoch: 3 [3840/60000(6%)]\t Loss: 103.386948\n",
      "Train epoch: 3 [5120/60000(9%)]\t Loss: 93.720062\n",
      "Train epoch: 3 [6400/60000(11%)]\t Loss: 100.600082\n",
      "Train epoch: 3 [7680/60000(13%)]\t Loss: 100.879837\n",
      "Train epoch: 3 [8960/60000(15%)]\t Loss: 96.876305\n",
      "Train epoch: 3 [10240/60000(17%)]\t Loss: 105.651779\n",
      "Train epoch: 3 [11520/60000(19%)]\t Loss: 106.873596\n",
      "Train epoch: 3 [12800/60000(21%)]\t Loss: 105.986969\n",
      "Train epoch: 3 [14080/60000(23%)]\t Loss: 100.922409\n",
      "Train epoch: 3 [15360/60000(26%)]\t Loss: 92.467545\n",
      "Train epoch: 3 [16640/60000(28%)]\t Loss: 100.254646\n",
      "Train epoch: 3 [17920/60000(30%)]\t Loss: 99.594994\n",
      "Train epoch: 3 [19200/60000(32%)]\t Loss: 99.778664\n",
      "Train epoch: 3 [20480/60000(34%)]\t Loss: 98.055481\n",
      "Train epoch: 3 [21760/60000(36%)]\t Loss: 100.753624\n",
      "Train epoch: 3 [23040/60000(38%)]\t Loss: 99.604767\n",
      "Train epoch: 3 [24320/60000(41%)]\t Loss: 97.621262\n",
      "Train epoch: 3 [25600/60000(43%)]\t Loss: 95.831985\n",
      "Train epoch: 3 [26880/60000(45%)]\t Loss: 102.393234\n",
      "Train epoch: 3 [28160/60000(47%)]\t Loss: 96.279350\n",
      "Train epoch: 3 [29440/60000(49%)]\t Loss: 96.919914\n",
      "Train epoch: 3 [30720/60000(51%)]\t Loss: 94.412186\n",
      "Train epoch: 3 [32000/60000(53%)]\t Loss: 95.470337\n",
      "Train epoch: 3 [33280/60000(55%)]\t Loss: 98.677673\n",
      "Train epoch: 3 [34560/60000(58%)]\t Loss: 99.878654\n",
      "Train epoch: 3 [35840/60000(60%)]\t Loss: 99.113419\n",
      "Train epoch: 3 [37120/60000(62%)]\t Loss: 94.910637\n",
      "Train epoch: 3 [38400/60000(64%)]\t Loss: 96.720009\n",
      "Train epoch: 3 [39680/60000(66%)]\t Loss: 98.372528\n",
      "Train epoch: 3 [40960/60000(68%)]\t Loss: 95.408295\n",
      "Train epoch: 3 [42240/60000(70%)]\t Loss: 94.155594\n",
      "Train epoch: 3 [43520/60000(72%)]\t Loss: 96.794739\n",
      "Train epoch: 3 [44800/60000(75%)]\t Loss: 91.046089\n",
      "Train epoch: 3 [46080/60000(77%)]\t Loss: 93.968330\n",
      "Train epoch: 3 [47360/60000(79%)]\t Loss: 97.067375\n",
      "Train epoch: 3 [48640/60000(81%)]\t Loss: 95.973053\n",
      "Train epoch: 3 [49920/60000(83%)]\t Loss: 93.746742\n",
      "Train epoch: 3 [51200/60000(85%)]\t Loss: 93.310287\n",
      "Train epoch: 3 [52480/60000(87%)]\t Loss: 95.287277\n",
      "Train epoch: 3 [53760/60000(90%)]\t Loss: 91.868965\n",
      "Train epoch: 3 [55040/60000(92%)]\t Loss: 94.731392\n",
      "Train epoch: 3 [56320/60000(94%)]\t Loss: 93.387909\n",
      "Train epoch: 3 [57600/60000(96%)]\t Loss: 95.391785\n",
      "Train epoch: 3 [58880/60000(98%)]\t Loss: 91.422653\n",
      "====> Epoch: 3 Average loss: 97.6018\n",
      "Train epoch: 4 [0/60000(0%)]\t Loss: 90.759071\n",
      "Train epoch: 4 [1280/60000(2%)]\t Loss: 92.450142\n",
      "Train epoch: 4 [2560/60000(4%)]\t Loss: 90.676949\n",
      "Train epoch: 4 [3840/60000(6%)]\t Loss: 95.182106\n",
      "Train epoch: 4 [5120/60000(9%)]\t Loss: 89.241272\n",
      "Train epoch: 4 [6400/60000(11%)]\t Loss: 92.942291\n",
      "Train epoch: 4 [7680/60000(13%)]\t Loss: 90.192078\n",
      "Train epoch: 4 [8960/60000(15%)]\t Loss: 90.580521\n",
      "Train epoch: 4 [10240/60000(17%)]\t Loss: 91.857315\n",
      "Train epoch: 4 [11520/60000(19%)]\t Loss: 90.585159\n",
      "Train epoch: 4 [12800/60000(21%)]\t Loss: 89.480110\n",
      "Train epoch: 4 [14080/60000(23%)]\t Loss: 95.125244\n",
      "Train epoch: 4 [15360/60000(26%)]\t Loss: 88.419930\n",
      "Train epoch: 4 [16640/60000(28%)]\t Loss: 94.385033\n",
      "Train epoch: 4 [17920/60000(30%)]\t Loss: 92.316925\n",
      "Train epoch: 4 [19200/60000(32%)]\t Loss: 87.521744\n",
      "Train epoch: 4 [20480/60000(34%)]\t Loss: 87.458839\n",
      "Train epoch: 4 [21760/60000(36%)]\t Loss: 90.010277\n",
      "Train epoch: 4 [23040/60000(38%)]\t Loss: 89.781738\n",
      "Train epoch: 4 [24320/60000(41%)]\t Loss: 88.305649\n",
      "Train epoch: 4 [25600/60000(43%)]\t Loss: 86.594536\n",
      "Train epoch: 4 [26880/60000(45%)]\t Loss: 87.970886\n",
      "Train epoch: 4 [28160/60000(47%)]\t Loss: 90.218048\n",
      "Train epoch: 4 [29440/60000(49%)]\t Loss: 93.256050\n",
      "Train epoch: 4 [30720/60000(51%)]\t Loss: 86.529129\n",
      "Train epoch: 4 [32000/60000(53%)]\t Loss: 89.618378\n",
      "Train epoch: 4 [33280/60000(55%)]\t Loss: 84.655418\n",
      "Train epoch: 4 [34560/60000(58%)]\t Loss: 86.892822\n",
      "Train epoch: 4 [35840/60000(60%)]\t Loss: 93.260635\n",
      "Train epoch: 4 [37120/60000(62%)]\t Loss: 89.133606\n",
      "Train epoch: 4 [38400/60000(64%)]\t Loss: 90.736824\n",
      "Train epoch: 4 [39680/60000(66%)]\t Loss: 86.536324\n",
      "Train epoch: 4 [40960/60000(68%)]\t Loss: 86.653542\n",
      "Train epoch: 4 [42240/60000(70%)]\t Loss: 84.426567\n",
      "Train epoch: 4 [43520/60000(72%)]\t Loss: 83.305626\n",
      "Train epoch: 4 [44800/60000(75%)]\t Loss: 87.781281\n",
      "Train epoch: 4 [46080/60000(77%)]\t Loss: 82.686523\n",
      "Train epoch: 4 [47360/60000(79%)]\t Loss: 86.386208\n",
      "Train epoch: 4 [48640/60000(81%)]\t Loss: 88.203430\n",
      "Train epoch: 4 [49920/60000(83%)]\t Loss: 86.246185\n",
      "Train epoch: 4 [51200/60000(85%)]\t Loss: 83.585892\n",
      "Train epoch: 4 [52480/60000(87%)]\t Loss: 86.396790\n",
      "Train epoch: 4 [53760/60000(90%)]\t Loss: 82.801010\n",
      "Train epoch: 4 [55040/60000(92%)]\t Loss: 82.875916\n",
      "Train epoch: 4 [56320/60000(94%)]\t Loss: 80.531487\n",
      "Train epoch: 4 [57600/60000(96%)]\t Loss: 83.662659\n",
      "Train epoch: 4 [58880/60000(98%)]\t Loss: 86.241997\n",
      "====> Epoch: 4 Average loss: 88.4706\n",
      "Train epoch: 5 [0/60000(0%)]\t Loss: 84.786057\n",
      "Train epoch: 5 [1280/60000(2%)]\t Loss: 82.941483\n",
      "Train epoch: 5 [2560/60000(4%)]\t Loss: 86.931679\n",
      "Train epoch: 5 [3840/60000(6%)]\t Loss: 86.326294\n",
      "Train epoch: 5 [5120/60000(9%)]\t Loss: 84.741005\n",
      "Train epoch: 5 [6400/60000(11%)]\t Loss: 84.655739\n",
      "Train epoch: 5 [7680/60000(13%)]\t Loss: 81.428886\n",
      "Train epoch: 5 [8960/60000(15%)]\t Loss: 85.457565\n",
      "Train epoch: 5 [10240/60000(17%)]\t Loss: 83.977966\n",
      "Train epoch: 5 [11520/60000(19%)]\t Loss: 85.774826\n",
      "Train epoch: 5 [12800/60000(21%)]\t Loss: 80.700134\n",
      "Train epoch: 5 [14080/60000(23%)]\t Loss: 86.405006\n",
      "Train epoch: 5 [15360/60000(26%)]\t Loss: 83.848671\n",
      "Train epoch: 5 [16640/60000(28%)]\t Loss: 81.335358\n",
      "Train epoch: 5 [17920/60000(30%)]\t Loss: 82.407127\n",
      "Train epoch: 5 [19200/60000(32%)]\t Loss: 82.475777\n",
      "Train epoch: 5 [20480/60000(34%)]\t Loss: 81.274460\n",
      "Train epoch: 5 [21760/60000(36%)]\t Loss: 86.226387\n",
      "Train epoch: 5 [23040/60000(38%)]\t Loss: 84.675621\n",
      "Train epoch: 5 [24320/60000(41%)]\t Loss: 83.929710\n",
      "Train epoch: 5 [25600/60000(43%)]\t Loss: 82.650063\n",
      "Train epoch: 5 [26880/60000(45%)]\t Loss: 83.278297\n",
      "Train epoch: 5 [28160/60000(47%)]\t Loss: 88.280678\n",
      "Train epoch: 5 [29440/60000(49%)]\t Loss: 82.163902\n",
      "Train epoch: 5 [30720/60000(51%)]\t Loss: 82.170670\n",
      "Train epoch: 5 [32000/60000(53%)]\t Loss: 82.259995\n",
      "Train epoch: 5 [33280/60000(55%)]\t Loss: 82.619217\n",
      "Train epoch: 5 [34560/60000(58%)]\t Loss: 81.861359\n",
      "Train epoch: 5 [35840/60000(60%)]\t Loss: 82.508118\n",
      "Train epoch: 5 [37120/60000(62%)]\t Loss: 80.327957\n",
      "Train epoch: 5 [38400/60000(64%)]\t Loss: 83.183693\n",
      "Train epoch: 5 [39680/60000(66%)]\t Loss: 83.853706\n",
      "Train epoch: 5 [40960/60000(68%)]\t Loss: 83.822479\n",
      "Train epoch: 5 [42240/60000(70%)]\t Loss: 80.784561\n",
      "Train epoch: 5 [43520/60000(72%)]\t Loss: 78.323097\n",
      "Train epoch: 5 [44800/60000(75%)]\t Loss: 82.653824\n",
      "Train epoch: 5 [46080/60000(77%)]\t Loss: 81.736221\n",
      "Train epoch: 5 [47360/60000(79%)]\t Loss: 80.692513\n",
      "Train epoch: 5 [48640/60000(81%)]\t Loss: 78.536369\n",
      "Train epoch: 5 [49920/60000(83%)]\t Loss: 84.337097\n",
      "Train epoch: 5 [51200/60000(85%)]\t Loss: 79.016663\n",
      "Train epoch: 5 [52480/60000(87%)]\t Loss: 78.624100\n",
      "Train epoch: 5 [53760/60000(90%)]\t Loss: 79.807159\n",
      "Train epoch: 5 [55040/60000(92%)]\t Loss: 80.019005\n",
      "Train epoch: 5 [56320/60000(94%)]\t Loss: 80.012848\n",
      "Train epoch: 5 [57600/60000(96%)]\t Loss: 76.481895\n",
      "Train epoch: 5 [58880/60000(98%)]\t Loss: 78.862099\n",
      "====> Epoch: 5 Average loss: 81.9237\n",
      "Train epoch: 6 [0/60000(0%)]\t Loss: 81.353661\n",
      "Train epoch: 6 [1280/60000(2%)]\t Loss: 81.064049\n",
      "Train epoch: 6 [2560/60000(4%)]\t Loss: 79.219696\n",
      "Train epoch: 6 [3840/60000(6%)]\t Loss: 79.208229\n",
      "Train epoch: 6 [5120/60000(9%)]\t Loss: 77.490334\n",
      "Train epoch: 6 [6400/60000(11%)]\t Loss: 81.622147\n",
      "Train epoch: 6 [7680/60000(13%)]\t Loss: 78.159149\n",
      "Train epoch: 6 [8960/60000(15%)]\t Loss: 81.788879\n",
      "Train epoch: 6 [10240/60000(17%)]\t Loss: 78.732391\n",
      "Train epoch: 6 [11520/60000(19%)]\t Loss: 79.345131\n",
      "Train epoch: 6 [12800/60000(21%)]\t Loss: 80.704956\n",
      "Train epoch: 6 [14080/60000(23%)]\t Loss: 79.929649\n",
      "Train epoch: 6 [15360/60000(26%)]\t Loss: 75.151321\n",
      "Train epoch: 6 [16640/60000(28%)]\t Loss: 76.182167\n",
      "Train epoch: 6 [17920/60000(30%)]\t Loss: 78.933456\n",
      "Train epoch: 6 [19200/60000(32%)]\t Loss: 78.316971\n",
      "Train epoch: 6 [20480/60000(34%)]\t Loss: 77.862343\n",
      "Train epoch: 6 [21760/60000(36%)]\t Loss: 75.229546\n",
      "Train epoch: 6 [23040/60000(38%)]\t Loss: 79.171524\n",
      "Train epoch: 6 [24320/60000(41%)]\t Loss: 78.567558\n",
      "Train epoch: 6 [25600/60000(43%)]\t Loss: 77.168213\n",
      "Train epoch: 6 [26880/60000(45%)]\t Loss: 77.177147\n",
      "Train epoch: 6 [28160/60000(47%)]\t Loss: 77.596687\n",
      "Train epoch: 6 [29440/60000(49%)]\t Loss: 76.342972\n",
      "Train epoch: 6 [30720/60000(51%)]\t Loss: 77.504234\n",
      "Train epoch: 6 [32000/60000(53%)]\t Loss: 80.015854\n",
      "Train epoch: 6 [33280/60000(55%)]\t Loss: 76.278481\n",
      "Train epoch: 6 [34560/60000(58%)]\t Loss: 76.666702\n",
      "Train epoch: 6 [35840/60000(60%)]\t Loss: 74.271004\n",
      "Train epoch: 6 [37120/60000(62%)]\t Loss: 76.396088\n",
      "Train epoch: 6 [38400/60000(64%)]\t Loss: 76.377411\n",
      "Train epoch: 6 [39680/60000(66%)]\t Loss: 76.560890\n",
      "Train epoch: 6 [40960/60000(68%)]\t Loss: 74.201408\n",
      "Train epoch: 6 [42240/60000(70%)]\t Loss: 75.770348\n",
      "Train epoch: 6 [43520/60000(72%)]\t Loss: 75.898010\n",
      "Train epoch: 6 [44800/60000(75%)]\t Loss: 73.914307\n",
      "Train epoch: 6 [46080/60000(77%)]\t Loss: 76.946747\n",
      "Train epoch: 6 [47360/60000(79%)]\t Loss: 75.750717\n",
      "Train epoch: 6 [48640/60000(81%)]\t Loss: 80.006203\n",
      "Train epoch: 6 [49920/60000(83%)]\t Loss: 76.124825\n",
      "Train epoch: 6 [51200/60000(85%)]\t Loss: 74.470306\n",
      "Train epoch: 6 [52480/60000(87%)]\t Loss: 76.898293\n",
      "Train epoch: 6 [53760/60000(90%)]\t Loss: 78.601334\n",
      "Train epoch: 6 [55040/60000(92%)]\t Loss: 73.463936\n",
      "Train epoch: 6 [56320/60000(94%)]\t Loss: 74.461365\n",
      "Train epoch: 6 [57600/60000(96%)]\t Loss: 74.199028\n",
      "Train epoch: 6 [58880/60000(98%)]\t Loss: 76.701408\n",
      "====> Epoch: 6 Average loss: 77.0383\n",
      "Train epoch: 7 [0/60000(0%)]\t Loss: 76.391785\n",
      "Train epoch: 7 [1280/60000(2%)]\t Loss: 76.042099\n",
      "Train epoch: 7 [2560/60000(4%)]\t Loss: 73.439468\n",
      "Train epoch: 7 [3840/60000(6%)]\t Loss: 75.133842\n",
      "Train epoch: 7 [5120/60000(9%)]\t Loss: 75.110283\n",
      "Train epoch: 7 [6400/60000(11%)]\t Loss: 73.684807\n",
      "Train epoch: 7 [7680/60000(13%)]\t Loss: 72.282951\n",
      "Train epoch: 7 [8960/60000(15%)]\t Loss: 73.663231\n",
      "Train epoch: 7 [10240/60000(17%)]\t Loss: 73.670059\n",
      "Train epoch: 7 [11520/60000(19%)]\t Loss: 72.089737\n",
      "Train epoch: 7 [12800/60000(21%)]\t Loss: 70.268227\n",
      "Train epoch: 7 [14080/60000(23%)]\t Loss: 77.402702\n",
      "Train epoch: 7 [15360/60000(26%)]\t Loss: 73.754570\n",
      "Train epoch: 7 [16640/60000(28%)]\t Loss: 72.412758\n",
      "Train epoch: 7 [17920/60000(30%)]\t Loss: 74.983734\n",
      "Train epoch: 7 [19200/60000(32%)]\t Loss: 75.318970\n",
      "Train epoch: 7 [20480/60000(34%)]\t Loss: 74.094810\n",
      "Train epoch: 7 [21760/60000(36%)]\t Loss: 72.079155\n",
      "Train epoch: 7 [23040/60000(38%)]\t Loss: 75.626968\n",
      "Train epoch: 7 [24320/60000(41%)]\t Loss: 74.237122\n",
      "Train epoch: 7 [25600/60000(43%)]\t Loss: 73.634460\n",
      "Train epoch: 7 [26880/60000(45%)]\t Loss: 73.571281\n",
      "Train epoch: 7 [28160/60000(47%)]\t Loss: 73.218872\n",
      "Train epoch: 7 [29440/60000(49%)]\t Loss: 72.617950\n",
      "Train epoch: 7 [30720/60000(51%)]\t Loss: 74.935158\n",
      "Train epoch: 7 [32000/60000(53%)]\t Loss: 72.953407\n",
      "Train epoch: 7 [33280/60000(55%)]\t Loss: 74.767593\n",
      "Train epoch: 7 [34560/60000(58%)]\t Loss: 71.176804\n",
      "Train epoch: 7 [35840/60000(60%)]\t Loss: 74.946884\n",
      "Train epoch: 7 [37120/60000(62%)]\t Loss: 72.939133\n",
      "Train epoch: 7 [38400/60000(64%)]\t Loss: 73.578896\n",
      "Train epoch: 7 [39680/60000(66%)]\t Loss: 73.288841\n",
      "Train epoch: 7 [40960/60000(68%)]\t Loss: 72.113113\n",
      "Train epoch: 7 [42240/60000(70%)]\t Loss: 72.987877\n",
      "Train epoch: 7 [43520/60000(72%)]\t Loss: 72.257401\n",
      "Train epoch: 7 [44800/60000(75%)]\t Loss: 71.046593\n",
      "Train epoch: 7 [46080/60000(77%)]\t Loss: 72.209427\n",
      "Train epoch: 7 [47360/60000(79%)]\t Loss: 72.536850\n",
      "Train epoch: 7 [48640/60000(81%)]\t Loss: 75.601349\n",
      "Train epoch: 7 [49920/60000(83%)]\t Loss: 69.639572\n",
      "Train epoch: 7 [51200/60000(85%)]\t Loss: 73.377518\n",
      "Train epoch: 7 [52480/60000(87%)]\t Loss: 71.958534\n",
      "Train epoch: 7 [53760/60000(90%)]\t Loss: 71.035416\n",
      "Train epoch: 7 [55040/60000(92%)]\t Loss: 71.153046\n",
      "Train epoch: 7 [56320/60000(94%)]\t Loss: 70.071014\n",
      "Train epoch: 7 [57600/60000(96%)]\t Loss: 70.358337\n",
      "Train epoch: 7 [58880/60000(98%)]\t Loss: 72.594437\n",
      "====> Epoch: 7 Average loss: 73.2422\n",
      "Train epoch: 8 [0/60000(0%)]\t Loss: 70.710495\n",
      "Train epoch: 8 [1280/60000(2%)]\t Loss: 67.785225\n",
      "Train epoch: 8 [2560/60000(4%)]\t Loss: 72.509758\n",
      "Train epoch: 8 [3840/60000(6%)]\t Loss: 70.693466\n",
      "Train epoch: 8 [5120/60000(9%)]\t Loss: 69.678101\n",
      "Train epoch: 8 [6400/60000(11%)]\t Loss: 76.199989\n",
      "Train epoch: 8 [7680/60000(13%)]\t Loss: 70.226151\n",
      "Train epoch: 8 [8960/60000(15%)]\t Loss: 72.161873\n",
      "Train epoch: 8 [10240/60000(17%)]\t Loss: 73.231071\n",
      "Train epoch: 8 [11520/60000(19%)]\t Loss: 71.915543\n",
      "Train epoch: 8 [12800/60000(21%)]\t Loss: 73.207390\n",
      "Train epoch: 8 [14080/60000(23%)]\t Loss: 71.066299\n",
      "Train epoch: 8 [15360/60000(26%)]\t Loss: 68.718704\n",
      "Train epoch: 8 [16640/60000(28%)]\t Loss: 69.249733\n",
      "Train epoch: 8 [17920/60000(30%)]\t Loss: 69.643517\n",
      "Train epoch: 8 [19200/60000(32%)]\t Loss: 72.369286\n",
      "Train epoch: 8 [20480/60000(34%)]\t Loss: 71.050705\n",
      "Train epoch: 8 [21760/60000(36%)]\t Loss: 73.215775\n",
      "Train epoch: 8 [23040/60000(38%)]\t Loss: 71.503799\n",
      "Train epoch: 8 [24320/60000(41%)]\t Loss: 72.509758\n",
      "Train epoch: 8 [25600/60000(43%)]\t Loss: 70.779297\n",
      "Train epoch: 8 [26880/60000(45%)]\t Loss: 70.008949\n",
      "Train epoch: 8 [28160/60000(47%)]\t Loss: 71.822403\n",
      "Train epoch: 8 [29440/60000(49%)]\t Loss: 70.097336\n",
      "Train epoch: 8 [30720/60000(51%)]\t Loss: 68.884415\n",
      "Train epoch: 8 [32000/60000(53%)]\t Loss: 69.414291\n",
      "Train epoch: 8 [33280/60000(55%)]\t Loss: 72.038841\n",
      "Train epoch: 8 [34560/60000(58%)]\t Loss: 70.136490\n",
      "Train epoch: 8 [35840/60000(60%)]\t Loss: 71.646919\n",
      "Train epoch: 8 [37120/60000(62%)]\t Loss: 70.221222\n",
      "Train epoch: 8 [38400/60000(64%)]\t Loss: 69.931686\n",
      "Train epoch: 8 [39680/60000(66%)]\t Loss: 68.856430\n",
      "Train epoch: 8 [40960/60000(68%)]\t Loss: 70.073990\n",
      "Train epoch: 8 [42240/60000(70%)]\t Loss: 68.887100\n",
      "Train epoch: 8 [43520/60000(72%)]\t Loss: 72.581551\n",
      "Train epoch: 8 [44800/60000(75%)]\t Loss: 69.168945\n",
      "Train epoch: 8 [46080/60000(77%)]\t Loss: 70.862091\n",
      "Train epoch: 8 [47360/60000(79%)]\t Loss: 70.166130\n",
      "Train epoch: 8 [48640/60000(81%)]\t Loss: 70.906799\n",
      "Train epoch: 8 [49920/60000(83%)]\t Loss: 69.589592\n",
      "Train epoch: 8 [51200/60000(85%)]\t Loss: 70.166290\n",
      "Train epoch: 8 [52480/60000(87%)]\t Loss: 68.652412\n",
      "Train epoch: 8 [53760/60000(90%)]\t Loss: 72.150253\n",
      "Train epoch: 8 [55040/60000(92%)]\t Loss: 68.022903\n",
      "Train epoch: 8 [56320/60000(94%)]\t Loss: 69.386360\n",
      "Train epoch: 8 [57600/60000(96%)]\t Loss: 66.077515\n",
      "Train epoch: 8 [58880/60000(98%)]\t Loss: 67.827225\n",
      "====> Epoch: 8 Average loss: 70.2102\n",
      "Train epoch: 9 [0/60000(0%)]\t Loss: 68.950203\n",
      "Train epoch: 9 [1280/60000(2%)]\t Loss: 67.058983\n",
      "Train epoch: 9 [2560/60000(4%)]\t Loss: 67.031387\n",
      "Train epoch: 9 [3840/60000(6%)]\t Loss: 70.390724\n",
      "Train epoch: 9 [5120/60000(9%)]\t Loss: 66.854767\n",
      "Train epoch: 9 [6400/60000(11%)]\t Loss: 66.832932\n",
      "Train epoch: 9 [7680/60000(13%)]\t Loss: 66.673653\n",
      "Train epoch: 9 [8960/60000(15%)]\t Loss: 70.488449\n",
      "Train epoch: 9 [10240/60000(17%)]\t Loss: 69.806671\n",
      "Train epoch: 9 [11520/60000(19%)]\t Loss: 67.605263\n",
      "Train epoch: 9 [12800/60000(21%)]\t Loss: 68.656624\n",
      "Train epoch: 9 [14080/60000(23%)]\t Loss: 69.894562\n",
      "Train epoch: 9 [15360/60000(26%)]\t Loss: 69.747429\n",
      "Train epoch: 9 [16640/60000(28%)]\t Loss: 68.317909\n",
      "Train epoch: 9 [17920/60000(30%)]\t Loss: 69.251587\n",
      "Train epoch: 9 [19200/60000(32%)]\t Loss: 69.761124\n",
      "Train epoch: 9 [20480/60000(34%)]\t Loss: 67.798813\n",
      "Train epoch: 9 [21760/60000(36%)]\t Loss: 67.105911\n",
      "Train epoch: 9 [23040/60000(38%)]\t Loss: 69.116814\n",
      "Train epoch: 9 [24320/60000(41%)]\t Loss: 71.434799\n",
      "Train epoch: 9 [25600/60000(43%)]\t Loss: 67.952469\n",
      "Train epoch: 9 [26880/60000(45%)]\t Loss: 69.477829\n",
      "Train epoch: 9 [28160/60000(47%)]\t Loss: 71.077003\n",
      "Train epoch: 9 [29440/60000(49%)]\t Loss: 67.563736\n",
      "Train epoch: 9 [30720/60000(51%)]\t Loss: 66.595726\n",
      "Train epoch: 9 [32000/60000(53%)]\t Loss: 64.332886\n",
      "Train epoch: 9 [33280/60000(55%)]\t Loss: 69.075760\n",
      "Train epoch: 9 [34560/60000(58%)]\t Loss: 70.979439\n",
      "Train epoch: 9 [35840/60000(60%)]\t Loss: 69.299095\n",
      "Train epoch: 9 [37120/60000(62%)]\t Loss: 69.033463\n",
      "Train epoch: 9 [38400/60000(64%)]\t Loss: 66.687973\n",
      "Train epoch: 9 [39680/60000(66%)]\t Loss: 66.792030\n",
      "Train epoch: 9 [40960/60000(68%)]\t Loss: 64.633476\n",
      "Train epoch: 9 [42240/60000(70%)]\t Loss: 66.332565\n",
      "Train epoch: 9 [43520/60000(72%)]\t Loss: 67.798622\n",
      "Train epoch: 9 [44800/60000(75%)]\t Loss: 67.509018\n",
      "Train epoch: 9 [46080/60000(77%)]\t Loss: 69.186493\n",
      "Train epoch: 9 [47360/60000(79%)]\t Loss: 66.628990\n",
      "Train epoch: 9 [48640/60000(81%)]\t Loss: 69.132851\n",
      "Train epoch: 9 [49920/60000(83%)]\t Loss: 64.594658\n",
      "Train epoch: 9 [51200/60000(85%)]\t Loss: 66.887077\n",
      "Train epoch: 9 [52480/60000(87%)]\t Loss: 66.313858\n",
      "Train epoch: 9 [53760/60000(90%)]\t Loss: 66.090691\n",
      "Train epoch: 9 [55040/60000(92%)]\t Loss: 68.285629\n",
      "Train epoch: 9 [56320/60000(94%)]\t Loss: 66.703972\n",
      "Train epoch: 9 [57600/60000(96%)]\t Loss: 66.070953\n",
      "Train epoch: 9 [58880/60000(98%)]\t Loss: 64.765251\n",
      "====> Epoch: 9 Average loss: 67.7638\n",
      "Train epoch: 10 [0/60000(0%)]\t Loss: 68.691605\n",
      "Train epoch: 10 [1280/60000(2%)]\t Loss: 66.488541\n",
      "Train epoch: 10 [2560/60000(4%)]\t Loss: 66.662773\n",
      "Train epoch: 10 [3840/60000(6%)]\t Loss: 65.770363\n",
      "Train epoch: 10 [5120/60000(9%)]\t Loss: 67.018799\n",
      "Train epoch: 10 [6400/60000(11%)]\t Loss: 65.825310\n",
      "Train epoch: 10 [7680/60000(13%)]\t Loss: 68.945969\n",
      "Train epoch: 10 [8960/60000(15%)]\t Loss: 64.793762\n",
      "Train epoch: 10 [10240/60000(17%)]\t Loss: 67.072334\n",
      "Train epoch: 10 [11520/60000(19%)]\t Loss: 66.349861\n",
      "Train epoch: 10 [12800/60000(21%)]\t Loss: 67.128174\n",
      "Train epoch: 10 [14080/60000(23%)]\t Loss: 65.452515\n",
      "Train epoch: 10 [15360/60000(26%)]\t Loss: 64.099060\n",
      "Train epoch: 10 [16640/60000(28%)]\t Loss: 64.644257\n",
      "Train epoch: 10 [17920/60000(30%)]\t Loss: 65.280983\n",
      "Train epoch: 10 [19200/60000(32%)]\t Loss: 67.411240\n",
      "Train epoch: 10 [20480/60000(34%)]\t Loss: 67.525032\n",
      "Train epoch: 10 [21760/60000(36%)]\t Loss: 66.513451\n",
      "Train epoch: 10 [23040/60000(38%)]\t Loss: 67.891586\n",
      "Train epoch: 10 [24320/60000(41%)]\t Loss: 65.677864\n",
      "Train epoch: 10 [25600/60000(43%)]\t Loss: 65.690300\n",
      "Train epoch: 10 [26880/60000(45%)]\t Loss: 66.031456\n",
      "Train epoch: 10 [28160/60000(47%)]\t Loss: 67.521156\n",
      "Train epoch: 10 [29440/60000(49%)]\t Loss: 64.907394\n",
      "Train epoch: 10 [30720/60000(51%)]\t Loss: 65.438713\n",
      "Train epoch: 10 [32000/60000(53%)]\t Loss: 65.807396\n",
      "Train epoch: 10 [33280/60000(55%)]\t Loss: 67.518814\n",
      "Train epoch: 10 [34560/60000(58%)]\t Loss: 65.132416\n",
      "Train epoch: 10 [35840/60000(60%)]\t Loss: 67.513130\n",
      "Train epoch: 10 [37120/60000(62%)]\t Loss: 64.377327\n",
      "Train epoch: 10 [38400/60000(64%)]\t Loss: 64.893242\n",
      "Train epoch: 10 [39680/60000(66%)]\t Loss: 64.752846\n",
      "Train epoch: 10 [40960/60000(68%)]\t Loss: 67.008820\n",
      "Train epoch: 10 [42240/60000(70%)]\t Loss: 64.200844\n",
      "Train epoch: 10 [43520/60000(72%)]\t Loss: 65.391991\n",
      "Train epoch: 10 [44800/60000(75%)]\t Loss: 64.312393\n",
      "Train epoch: 10 [46080/60000(77%)]\t Loss: 64.070053\n",
      "Train epoch: 10 [47360/60000(79%)]\t Loss: 66.221924\n",
      "Train epoch: 10 [48640/60000(81%)]\t Loss: 66.569786\n",
      "Train epoch: 10 [49920/60000(83%)]\t Loss: 63.978535\n",
      "Train epoch: 10 [51200/60000(85%)]\t Loss: 64.505424\n",
      "Train epoch: 10 [52480/60000(87%)]\t Loss: 64.645439\n",
      "Train epoch: 10 [53760/60000(90%)]\t Loss: 68.396660\n",
      "Train epoch: 10 [55040/60000(92%)]\t Loss: 63.171814\n",
      "Train epoch: 10 [56320/60000(94%)]\t Loss: 62.217594\n",
      "Train epoch: 10 [57600/60000(96%)]\t Loss: 66.275482\n",
      "Train epoch: 10 [58880/60000(98%)]\t Loss: 65.568542\n",
      "====> Epoch: 10 Average loss: 65.7481\n",
      "Train epoch: 11 [0/60000(0%)]\t Loss: 65.499756\n",
      "Train epoch: 11 [1280/60000(2%)]\t Loss: 66.382942\n",
      "Train epoch: 11 [2560/60000(4%)]\t Loss: 62.365601\n",
      "Train epoch: 11 [3840/60000(6%)]\t Loss: 64.230659\n",
      "Train epoch: 11 [5120/60000(9%)]\t Loss: 64.972191\n",
      "Train epoch: 11 [6400/60000(11%)]\t Loss: 64.590759\n",
      "Train epoch: 11 [7680/60000(13%)]\t Loss: 63.947113\n",
      "Train epoch: 11 [8960/60000(15%)]\t Loss: 64.187019\n",
      "Train epoch: 11 [10240/60000(17%)]\t Loss: 65.168625\n",
      "Train epoch: 11 [11520/60000(19%)]\t Loss: 63.192337\n",
      "Train epoch: 11 [12800/60000(21%)]\t Loss: 63.562119\n",
      "Train epoch: 11 [14080/60000(23%)]\t Loss: 62.732864\n",
      "Train epoch: 11 [15360/60000(26%)]\t Loss: 62.846931\n",
      "Train epoch: 11 [16640/60000(28%)]\t Loss: 59.424656\n",
      "Train epoch: 11 [17920/60000(30%)]\t Loss: 64.028473\n",
      "Train epoch: 11 [19200/60000(32%)]\t Loss: 60.745117\n",
      "Train epoch: 11 [20480/60000(34%)]\t Loss: 65.051262\n",
      "Train epoch: 11 [21760/60000(36%)]\t Loss: 64.970268\n",
      "Train epoch: 11 [23040/60000(38%)]\t Loss: 64.133148\n",
      "Train epoch: 11 [24320/60000(41%)]\t Loss: 65.227295\n",
      "Train epoch: 11 [25600/60000(43%)]\t Loss: 62.428814\n",
      "Train epoch: 11 [26880/60000(45%)]\t Loss: 61.914143\n",
      "Train epoch: 11 [28160/60000(47%)]\t Loss: 63.245956\n",
      "Train epoch: 11 [29440/60000(49%)]\t Loss: 64.051735\n",
      "Train epoch: 11 [30720/60000(51%)]\t Loss: 62.445240\n",
      "Train epoch: 11 [32000/60000(53%)]\t Loss: 64.778404\n",
      "Train epoch: 11 [33280/60000(55%)]\t Loss: 64.853050\n",
      "Train epoch: 11 [34560/60000(58%)]\t Loss: 62.934097\n",
      "Train epoch: 11 [35840/60000(60%)]\t Loss: 65.895592\n",
      "Train epoch: 11 [37120/60000(62%)]\t Loss: 64.621864\n",
      "Train epoch: 11 [38400/60000(64%)]\t Loss: 63.090046\n",
      "Train epoch: 11 [39680/60000(66%)]\t Loss: 64.267601\n",
      "Train epoch: 11 [40960/60000(68%)]\t Loss: 62.727093\n",
      "Train epoch: 11 [42240/60000(70%)]\t Loss: 61.977409\n",
      "Train epoch: 11 [43520/60000(72%)]\t Loss: 64.397026\n",
      "Train epoch: 11 [44800/60000(75%)]\t Loss: 64.266136\n",
      "Train epoch: 11 [46080/60000(77%)]\t Loss: 63.488586\n",
      "Train epoch: 11 [47360/60000(79%)]\t Loss: 63.865643\n",
      "Train epoch: 11 [48640/60000(81%)]\t Loss: 63.355637\n",
      "Train epoch: 11 [49920/60000(83%)]\t Loss: 62.985420\n",
      "Train epoch: 11 [51200/60000(85%)]\t Loss: 62.368656\n",
      "Train epoch: 11 [52480/60000(87%)]\t Loss: 65.684189\n",
      "Train epoch: 11 [53760/60000(90%)]\t Loss: 65.527527\n",
      "Train epoch: 11 [55040/60000(92%)]\t Loss: 63.199165\n",
      "Train epoch: 11 [56320/60000(94%)]\t Loss: 60.194607\n",
      "Train epoch: 11 [57600/60000(96%)]\t Loss: 64.272186\n",
      "Train epoch: 11 [58880/60000(98%)]\t Loss: 63.240211\n",
      "====> Epoch: 11 Average loss: 64.0727\n",
      "Train epoch: 12 [0/60000(0%)]\t Loss: 60.928909\n",
      "Train epoch: 12 [1280/60000(2%)]\t Loss: 61.797760\n",
      "Train epoch: 12 [2560/60000(4%)]\t Loss: 64.093613\n",
      "Train epoch: 12 [3840/60000(6%)]\t Loss: 61.239948\n",
      "Train epoch: 12 [5120/60000(9%)]\t Loss: 62.709656\n",
      "Train epoch: 12 [6400/60000(11%)]\t Loss: 63.137657\n",
      "Train epoch: 12 [7680/60000(13%)]\t Loss: 62.585476\n",
      "Train epoch: 12 [8960/60000(15%)]\t Loss: 66.028488\n",
      "Train epoch: 12 [10240/60000(17%)]\t Loss: 63.774109\n",
      "Train epoch: 12 [11520/60000(19%)]\t Loss: 62.807442\n",
      "Train epoch: 12 [12800/60000(21%)]\t Loss: 59.982857\n",
      "Train epoch: 12 [14080/60000(23%)]\t Loss: 61.806068\n",
      "Train epoch: 12 [15360/60000(26%)]\t Loss: 63.787376\n",
      "Train epoch: 12 [16640/60000(28%)]\t Loss: 63.317055\n",
      "Train epoch: 12 [17920/60000(30%)]\t Loss: 62.434029\n",
      "Train epoch: 12 [19200/60000(32%)]\t Loss: 61.370590\n",
      "Train epoch: 12 [20480/60000(34%)]\t Loss: 61.654835\n",
      "Train epoch: 12 [21760/60000(36%)]\t Loss: 61.763557\n",
      "Train epoch: 12 [23040/60000(38%)]\t Loss: 62.350574\n",
      "Train epoch: 12 [24320/60000(41%)]\t Loss: 61.865784\n",
      "Train epoch: 12 [25600/60000(43%)]\t Loss: 62.816330\n",
      "Train epoch: 12 [26880/60000(45%)]\t Loss: 62.178558\n",
      "Train epoch: 12 [28160/60000(47%)]\t Loss: 64.949593\n",
      "Train epoch: 12 [29440/60000(49%)]\t Loss: 63.053650\n",
      "Train epoch: 12 [30720/60000(51%)]\t Loss: 63.617672\n",
      "Train epoch: 12 [32000/60000(53%)]\t Loss: 60.168827\n",
      "Train epoch: 12 [33280/60000(55%)]\t Loss: 63.082771\n",
      "Train epoch: 12 [34560/60000(58%)]\t Loss: 65.731285\n",
      "Train epoch: 12 [35840/60000(60%)]\t Loss: 63.656448\n",
      "Train epoch: 12 [37120/60000(62%)]\t Loss: 62.129673\n",
      "Train epoch: 12 [38400/60000(64%)]\t Loss: 62.095463\n",
      "Train epoch: 12 [39680/60000(66%)]\t Loss: 61.846279\n",
      "Train epoch: 12 [40960/60000(68%)]\t Loss: 61.559792\n",
      "Train epoch: 12 [42240/60000(70%)]\t Loss: 60.391758\n",
      "Train epoch: 12 [43520/60000(72%)]\t Loss: 61.857346\n",
      "Train epoch: 12 [44800/60000(75%)]\t Loss: 62.794773\n",
      "Train epoch: 12 [46080/60000(77%)]\t Loss: 59.184322\n",
      "Train epoch: 12 [47360/60000(79%)]\t Loss: 61.711079\n",
      "Train epoch: 12 [48640/60000(81%)]\t Loss: 63.164322\n",
      "Train epoch: 12 [49920/60000(83%)]\t Loss: 62.371517\n",
      "Train epoch: 12 [51200/60000(85%)]\t Loss: 61.727715\n",
      "Train epoch: 12 [52480/60000(87%)]\t Loss: 62.857754\n",
      "Train epoch: 12 [53760/60000(90%)]\t Loss: 61.701141\n",
      "Train epoch: 12 [55040/60000(92%)]\t Loss: 62.159866\n",
      "Train epoch: 12 [56320/60000(94%)]\t Loss: 62.287106\n",
      "Train epoch: 12 [57600/60000(96%)]\t Loss: 63.185810\n",
      "Train epoch: 12 [58880/60000(98%)]\t Loss: 60.031963\n",
      "====> Epoch: 12 Average loss: 62.6624\n",
      "Train epoch: 13 [0/60000(0%)]\t Loss: 61.273415\n",
      "Train epoch: 13 [1280/60000(2%)]\t Loss: 64.626823\n",
      "Train epoch: 13 [2560/60000(4%)]\t Loss: 61.041870\n",
      "Train epoch: 13 [3840/60000(6%)]\t Loss: 61.993683\n",
      "Train epoch: 13 [5120/60000(9%)]\t Loss: 60.583965\n",
      "Train epoch: 13 [6400/60000(11%)]\t Loss: 63.121395\n",
      "Train epoch: 13 [7680/60000(13%)]\t Loss: 61.169308\n",
      "Train epoch: 13 [8960/60000(15%)]\t Loss: 64.051598\n",
      "Train epoch: 13 [10240/60000(17%)]\t Loss: 63.352127\n",
      "Train epoch: 13 [11520/60000(19%)]\t Loss: 62.657722\n",
      "Train epoch: 13 [12800/60000(21%)]\t Loss: 63.744797\n",
      "Train epoch: 13 [14080/60000(23%)]\t Loss: 60.916504\n",
      "Train epoch: 13 [15360/60000(26%)]\t Loss: 60.482647\n",
      "Train epoch: 13 [16640/60000(28%)]\t Loss: 62.329563\n",
      "Train epoch: 13 [17920/60000(30%)]\t Loss: 62.097847\n",
      "Train epoch: 13 [19200/60000(32%)]\t Loss: 60.784847\n",
      "Train epoch: 13 [20480/60000(34%)]\t Loss: 60.664112\n",
      "Train epoch: 13 [21760/60000(36%)]\t Loss: 60.831490\n",
      "Train epoch: 13 [23040/60000(38%)]\t Loss: 59.724274\n",
      "Train epoch: 13 [24320/60000(41%)]\t Loss: 63.140278\n",
      "Train epoch: 13 [25600/60000(43%)]\t Loss: 61.590706\n",
      "Train epoch: 13 [26880/60000(45%)]\t Loss: 64.396515\n",
      "Train epoch: 13 [28160/60000(47%)]\t Loss: 61.967094\n",
      "Train epoch: 13 [29440/60000(49%)]\t Loss: 62.245659\n",
      "Train epoch: 13 [30720/60000(51%)]\t Loss: 62.020264\n",
      "Train epoch: 13 [32000/60000(53%)]\t Loss: 61.192905\n",
      "Train epoch: 13 [33280/60000(55%)]\t Loss: 61.545540\n",
      "Train epoch: 13 [34560/60000(58%)]\t Loss: 62.283180\n",
      "Train epoch: 13 [35840/60000(60%)]\t Loss: 59.610565\n",
      "Train epoch: 13 [37120/60000(62%)]\t Loss: 63.352116\n",
      "Train epoch: 13 [38400/60000(64%)]\t Loss: 62.704254\n",
      "Train epoch: 13 [39680/60000(66%)]\t Loss: 61.862061\n",
      "Train epoch: 13 [40960/60000(68%)]\t Loss: 62.147091\n",
      "Train epoch: 13 [42240/60000(70%)]\t Loss: 59.325989\n",
      "Train epoch: 13 [43520/60000(72%)]\t Loss: 58.393822\n",
      "Train epoch: 13 [44800/60000(75%)]\t Loss: 59.789227\n",
      "Train epoch: 13 [46080/60000(77%)]\t Loss: 61.280373\n",
      "Train epoch: 13 [47360/60000(79%)]\t Loss: 61.889359\n",
      "Train epoch: 13 [48640/60000(81%)]\t Loss: 59.892925\n",
      "Train epoch: 13 [49920/60000(83%)]\t Loss: 61.164337\n",
      "Train epoch: 13 [51200/60000(85%)]\t Loss: 60.667488\n",
      "Train epoch: 13 [52480/60000(87%)]\t Loss: 59.352764\n",
      "Train epoch: 13 [53760/60000(90%)]\t Loss: 60.485481\n",
      "Train epoch: 13 [55040/60000(92%)]\t Loss: 61.984421\n",
      "Train epoch: 13 [56320/60000(94%)]\t Loss: 59.592690\n",
      "Train epoch: 13 [57600/60000(96%)]\t Loss: 58.995911\n",
      "Train epoch: 13 [58880/60000(98%)]\t Loss: 58.340328\n",
      "====> Epoch: 13 Average loss: 61.4756\n",
      "Train epoch: 14 [0/60000(0%)]\t Loss: 58.745903\n",
      "Train epoch: 14 [1280/60000(2%)]\t Loss: 62.398617\n",
      "Train epoch: 14 [2560/60000(4%)]\t Loss: 61.188675\n",
      "Train epoch: 14 [3840/60000(6%)]\t Loss: 58.364944\n",
      "Train epoch: 14 [5120/60000(9%)]\t Loss: 63.529118\n",
      "Train epoch: 14 [6400/60000(11%)]\t Loss: 60.739300\n",
      "Train epoch: 14 [7680/60000(13%)]\t Loss: 61.246300\n",
      "Train epoch: 14 [8960/60000(15%)]\t Loss: 61.185932\n",
      "Train epoch: 14 [10240/60000(17%)]\t Loss: 60.049061\n",
      "Train epoch: 14 [11520/60000(19%)]\t Loss: 61.907104\n",
      "Train epoch: 14 [12800/60000(21%)]\t Loss: 61.919193\n",
      "Train epoch: 14 [14080/60000(23%)]\t Loss: 59.754555\n",
      "Train epoch: 14 [15360/60000(26%)]\t Loss: 62.928596\n",
      "Train epoch: 14 [16640/60000(28%)]\t Loss: 61.642010\n",
      "Train epoch: 14 [17920/60000(30%)]\t Loss: 60.246376\n",
      "Train epoch: 14 [19200/60000(32%)]\t Loss: 63.443283\n",
      "Train epoch: 14 [20480/60000(34%)]\t Loss: 61.254398\n",
      "Train epoch: 14 [21760/60000(36%)]\t Loss: 62.295944\n",
      "Train epoch: 14 [23040/60000(38%)]\t Loss: 60.283360\n",
      "Train epoch: 14 [24320/60000(41%)]\t Loss: 61.995388\n",
      "Train epoch: 14 [25600/60000(43%)]\t Loss: 60.287498\n",
      "Train epoch: 14 [26880/60000(45%)]\t Loss: 60.890751\n",
      "Train epoch: 14 [28160/60000(47%)]\t Loss: 61.143444\n",
      "Train epoch: 14 [29440/60000(49%)]\t Loss: 62.513149\n",
      "Train epoch: 14 [30720/60000(51%)]\t Loss: 62.362816\n",
      "Train epoch: 14 [32000/60000(53%)]\t Loss: 61.650909\n",
      "Train epoch: 14 [33280/60000(55%)]\t Loss: 59.023235\n",
      "Train epoch: 14 [34560/60000(58%)]\t Loss: 57.749367\n",
      "Train epoch: 14 [35840/60000(60%)]\t Loss: 59.203053\n",
      "Train epoch: 14 [37120/60000(62%)]\t Loss: 59.907856\n",
      "Train epoch: 14 [38400/60000(64%)]\t Loss: 59.837883\n",
      "Train epoch: 14 [39680/60000(66%)]\t Loss: 59.570335\n",
      "Train epoch: 14 [40960/60000(68%)]\t Loss: 60.501495\n",
      "Train epoch: 14 [42240/60000(70%)]\t Loss: 60.857780\n",
      "Train epoch: 14 [43520/60000(72%)]\t Loss: 61.267155\n",
      "Train epoch: 14 [44800/60000(75%)]\t Loss: 59.211304\n",
      "Train epoch: 14 [46080/60000(77%)]\t Loss: 59.416897\n",
      "Train epoch: 14 [47360/60000(79%)]\t Loss: 61.990070\n",
      "Train epoch: 14 [48640/60000(81%)]\t Loss: 60.412624\n",
      "Train epoch: 14 [49920/60000(83%)]\t Loss: 60.392128\n",
      "Train epoch: 14 [51200/60000(85%)]\t Loss: 61.574886\n",
      "Train epoch: 14 [52480/60000(87%)]\t Loss: 60.184040\n",
      "Train epoch: 14 [53760/60000(90%)]\t Loss: 62.171803\n",
      "Train epoch: 14 [55040/60000(92%)]\t Loss: 58.910744\n",
      "Train epoch: 14 [56320/60000(94%)]\t Loss: 58.830368\n",
      "Train epoch: 14 [57600/60000(96%)]\t Loss: 59.424747\n",
      "Train epoch: 14 [58880/60000(98%)]\t Loss: 59.327618\n",
      "====> Epoch: 14 Average loss: 60.4644\n",
      "Train epoch: 15 [0/60000(0%)]\t Loss: 61.500385\n",
      "Train epoch: 15 [1280/60000(2%)]\t Loss: 59.812084\n",
      "Train epoch: 15 [2560/60000(4%)]\t Loss: 59.293713\n",
      "Train epoch: 15 [3840/60000(6%)]\t Loss: 59.353100\n",
      "Train epoch: 15 [5120/60000(9%)]\t Loss: 59.266151\n",
      "Train epoch: 15 [6400/60000(11%)]\t Loss: 58.094486\n",
      "Train epoch: 15 [7680/60000(13%)]\t Loss: 59.525196\n",
      "Train epoch: 15 [8960/60000(15%)]\t Loss: 57.904209\n",
      "Train epoch: 15 [10240/60000(17%)]\t Loss: 61.436878\n",
      "Train epoch: 15 [11520/60000(19%)]\t Loss: 59.198059\n",
      "Train epoch: 15 [12800/60000(21%)]\t Loss: 62.639145\n",
      "Train epoch: 15 [14080/60000(23%)]\t Loss: 59.315228\n",
      "Train epoch: 15 [15360/60000(26%)]\t Loss: 59.227699\n",
      "Train epoch: 15 [16640/60000(28%)]\t Loss: 59.826557\n",
      "Train epoch: 15 [17920/60000(30%)]\t Loss: 58.892910\n",
      "Train epoch: 15 [19200/60000(32%)]\t Loss: 58.960083\n",
      "Train epoch: 15 [20480/60000(34%)]\t Loss: 60.350410\n",
      "Train epoch: 15 [21760/60000(36%)]\t Loss: 56.904400\n",
      "Train epoch: 15 [23040/60000(38%)]\t Loss: 60.195538\n",
      "Train epoch: 15 [24320/60000(41%)]\t Loss: 56.885796\n",
      "Train epoch: 15 [25600/60000(43%)]\t Loss: 58.593433\n",
      "Train epoch: 15 [26880/60000(45%)]\t Loss: 59.830986\n",
      "Train epoch: 15 [28160/60000(47%)]\t Loss: 58.559284\n",
      "Train epoch: 15 [29440/60000(49%)]\t Loss: 59.950844\n",
      "Train epoch: 15 [30720/60000(51%)]\t Loss: 59.308506\n",
      "Train epoch: 15 [32000/60000(53%)]\t Loss: 61.318161\n",
      "Train epoch: 15 [33280/60000(55%)]\t Loss: 59.381485\n",
      "Train epoch: 15 [34560/60000(58%)]\t Loss: 59.554455\n",
      "Train epoch: 15 [35840/60000(60%)]\t Loss: 60.516685\n",
      "Train epoch: 15 [37120/60000(62%)]\t Loss: 58.898178\n",
      "Train epoch: 15 [38400/60000(64%)]\t Loss: 56.393730\n",
      "Train epoch: 15 [39680/60000(66%)]\t Loss: 56.440083\n",
      "Train epoch: 15 [40960/60000(68%)]\t Loss: 59.111252\n",
      "Train epoch: 15 [42240/60000(70%)]\t Loss: 62.401535\n",
      "Train epoch: 15 [43520/60000(72%)]\t Loss: 59.029408\n",
      "Train epoch: 15 [44800/60000(75%)]\t Loss: 59.541782\n",
      "Train epoch: 15 [46080/60000(77%)]\t Loss: 59.284031\n",
      "Train epoch: 15 [47360/60000(79%)]\t Loss: 57.650826\n",
      "Train epoch: 15 [48640/60000(81%)]\t Loss: 60.707218\n",
      "Train epoch: 15 [49920/60000(83%)]\t Loss: 58.371590\n",
      "Train epoch: 15 [51200/60000(85%)]\t Loss: 58.630985\n",
      "Train epoch: 15 [52480/60000(87%)]\t Loss: 59.497852\n",
      "Train epoch: 15 [53760/60000(90%)]\t Loss: 60.223526\n",
      "Train epoch: 15 [55040/60000(92%)]\t Loss: 58.226269\n",
      "Train epoch: 15 [56320/60000(94%)]\t Loss: 58.341427\n",
      "Train epoch: 15 [57600/60000(96%)]\t Loss: 59.199341\n",
      "Train epoch: 15 [58880/60000(98%)]\t Loss: 58.979633\n",
      "====> Epoch: 15 Average loss: 59.5968\n",
      "Train epoch: 16 [0/60000(0%)]\t Loss: 58.882195\n",
      "Train epoch: 16 [1280/60000(2%)]\t Loss: 60.239281\n",
      "Train epoch: 16 [2560/60000(4%)]\t Loss: 59.024223\n",
      "Train epoch: 16 [3840/60000(6%)]\t Loss: 59.375187\n",
      "Train epoch: 16 [5120/60000(9%)]\t Loss: 59.389988\n",
      "Train epoch: 16 [6400/60000(11%)]\t Loss: 58.812012\n",
      "Train epoch: 16 [7680/60000(13%)]\t Loss: 60.407394\n",
      "Train epoch: 16 [8960/60000(15%)]\t Loss: 59.263035\n",
      "Train epoch: 16 [10240/60000(17%)]\t Loss: 58.050854\n",
      "Train epoch: 16 [11520/60000(19%)]\t Loss: 59.058853\n",
      "Train epoch: 16 [12800/60000(21%)]\t Loss: 58.346516\n",
      "Train epoch: 16 [14080/60000(23%)]\t Loss: 58.264664\n",
      "Train epoch: 16 [15360/60000(26%)]\t Loss: 59.489697\n",
      "Train epoch: 16 [16640/60000(28%)]\t Loss: 59.365566\n",
      "Train epoch: 16 [17920/60000(30%)]\t Loss: 59.269638\n",
      "Train epoch: 16 [19200/60000(32%)]\t Loss: 57.329910\n",
      "Train epoch: 16 [20480/60000(34%)]\t Loss: 58.211235\n",
      "Train epoch: 16 [21760/60000(36%)]\t Loss: 56.449619\n",
      "Train epoch: 16 [23040/60000(38%)]\t Loss: 57.298874\n",
      "Train epoch: 16 [24320/60000(41%)]\t Loss: 55.754589\n",
      "Train epoch: 16 [25600/60000(43%)]\t Loss: 59.995895\n",
      "Train epoch: 16 [26880/60000(45%)]\t Loss: 60.631668\n",
      "Train epoch: 16 [28160/60000(47%)]\t Loss: 57.272560\n",
      "Train epoch: 16 [29440/60000(49%)]\t Loss: 58.346466\n",
      "Train epoch: 16 [30720/60000(51%)]\t Loss: 57.600033\n",
      "Train epoch: 16 [32000/60000(53%)]\t Loss: 60.039612\n",
      "Train epoch: 16 [33280/60000(55%)]\t Loss: 57.517033\n",
      "Train epoch: 16 [34560/60000(58%)]\t Loss: 59.126171\n",
      "Train epoch: 16 [35840/60000(60%)]\t Loss: 59.095924\n",
      "Train epoch: 16 [37120/60000(62%)]\t Loss: 57.281860\n",
      "Train epoch: 16 [38400/60000(64%)]\t Loss: 58.905544\n",
      "Train epoch: 16 [39680/60000(66%)]\t Loss: 60.168083\n",
      "Train epoch: 16 [40960/60000(68%)]\t Loss: 59.315849\n",
      "Train epoch: 16 [42240/60000(70%)]\t Loss: 59.869148\n",
      "Train epoch: 16 [43520/60000(72%)]\t Loss: 57.890926\n",
      "Train epoch: 16 [44800/60000(75%)]\t Loss: 58.143661\n",
      "Train epoch: 16 [46080/60000(77%)]\t Loss: 61.079475\n",
      "Train epoch: 16 [47360/60000(79%)]\t Loss: 59.256920\n",
      "Train epoch: 16 [48640/60000(81%)]\t Loss: 58.281456\n",
      "Train epoch: 16 [49920/60000(83%)]\t Loss: 58.056206\n",
      "Train epoch: 16 [51200/60000(85%)]\t Loss: 56.727776\n",
      "Train epoch: 16 [52480/60000(87%)]\t Loss: 59.409832\n",
      "Train epoch: 16 [53760/60000(90%)]\t Loss: 56.263222\n",
      "Train epoch: 16 [55040/60000(92%)]\t Loss: 59.750965\n",
      "Train epoch: 16 [56320/60000(94%)]\t Loss: 61.008923\n",
      "Train epoch: 16 [57600/60000(96%)]\t Loss: 58.230740\n",
      "Train epoch: 16 [58880/60000(98%)]\t Loss: 57.775410\n",
      "====> Epoch: 16 Average loss: 58.8580\n",
      "Train epoch: 17 [0/60000(0%)]\t Loss: 56.769161\n",
      "Train epoch: 17 [1280/60000(2%)]\t Loss: 57.089226\n",
      "Train epoch: 17 [2560/60000(4%)]\t Loss: 57.601940\n",
      "Train epoch: 17 [3840/60000(6%)]\t Loss: 58.079735\n",
      "Train epoch: 17 [5120/60000(9%)]\t Loss: 56.232914\n",
      "Train epoch: 17 [6400/60000(11%)]\t Loss: 58.710102\n",
      "Train epoch: 17 [7680/60000(13%)]\t Loss: 57.283012\n",
      "Train epoch: 17 [8960/60000(15%)]\t Loss: 56.245804\n",
      "Train epoch: 17 [10240/60000(17%)]\t Loss: 59.641945\n",
      "Train epoch: 17 [11520/60000(19%)]\t Loss: 59.839054\n",
      "Train epoch: 17 [12800/60000(21%)]\t Loss: 56.508881\n",
      "Train epoch: 17 [14080/60000(23%)]\t Loss: 58.653095\n",
      "Train epoch: 17 [15360/60000(26%)]\t Loss: 57.776371\n",
      "Train epoch: 17 [16640/60000(28%)]\t Loss: 57.824249\n",
      "Train epoch: 17 [17920/60000(30%)]\t Loss: 57.790142\n",
      "Train epoch: 17 [19200/60000(32%)]\t Loss: 57.444279\n",
      "Train epoch: 17 [20480/60000(34%)]\t Loss: 58.749321\n",
      "Train epoch: 17 [21760/60000(36%)]\t Loss: 57.542507\n",
      "Train epoch: 17 [23040/60000(38%)]\t Loss: 59.978645\n",
      "Train epoch: 17 [24320/60000(41%)]\t Loss: 58.025417\n",
      "Train epoch: 17 [25600/60000(43%)]\t Loss: 56.865387\n",
      "Train epoch: 17 [26880/60000(45%)]\t Loss: 59.878201\n",
      "Train epoch: 17 [28160/60000(47%)]\t Loss: 56.629337\n",
      "Train epoch: 17 [29440/60000(49%)]\t Loss: 61.200901\n",
      "Train epoch: 17 [30720/60000(51%)]\t Loss: 58.214157\n",
      "Train epoch: 17 [32000/60000(53%)]\t Loss: 59.675468\n",
      "Train epoch: 17 [33280/60000(55%)]\t Loss: 56.373825\n",
      "Train epoch: 17 [34560/60000(58%)]\t Loss: 57.681683\n",
      "Train epoch: 17 [35840/60000(60%)]\t Loss: 57.119854\n",
      "Train epoch: 17 [37120/60000(62%)]\t Loss: 55.678341\n",
      "Train epoch: 17 [38400/60000(64%)]\t Loss: 56.866119\n",
      "Train epoch: 17 [39680/60000(66%)]\t Loss: 58.049850\n",
      "Train epoch: 17 [40960/60000(68%)]\t Loss: 59.639843\n",
      "Train epoch: 17 [42240/60000(70%)]\t Loss: 58.389275\n",
      "Train epoch: 17 [43520/60000(72%)]\t Loss: 56.022877\n",
      "Train epoch: 17 [44800/60000(75%)]\t Loss: 56.498425\n",
      "Train epoch: 17 [46080/60000(77%)]\t Loss: 60.436993\n",
      "Train epoch: 17 [47360/60000(79%)]\t Loss: 58.003677\n",
      "Train epoch: 17 [48640/60000(81%)]\t Loss: 57.283028\n",
      "Train epoch: 17 [49920/60000(83%)]\t Loss: 55.772938\n",
      "Train epoch: 17 [51200/60000(85%)]\t Loss: 58.486988\n",
      "Train epoch: 17 [52480/60000(87%)]\t Loss: 61.666283\n",
      "Train epoch: 17 [53760/60000(90%)]\t Loss: 60.502731\n",
      "Train epoch: 17 [55040/60000(92%)]\t Loss: 58.278141\n",
      "Train epoch: 17 [56320/60000(94%)]\t Loss: 56.852142\n",
      "Train epoch: 17 [57600/60000(96%)]\t Loss: 59.310001\n",
      "Train epoch: 17 [58880/60000(98%)]\t Loss: 54.530300\n",
      "====> Epoch: 17 Average loss: 58.2219\n",
      "Train epoch: 18 [0/60000(0%)]\t Loss: 59.312000\n",
      "Train epoch: 18 [1280/60000(2%)]\t Loss: 58.770985\n",
      "Train epoch: 18 [2560/60000(4%)]\t Loss: 57.595123\n",
      "Train epoch: 18 [3840/60000(6%)]\t Loss: 57.085094\n",
      "Train epoch: 18 [5120/60000(9%)]\t Loss: 58.172836\n",
      "Train epoch: 18 [6400/60000(11%)]\t Loss: 58.549572\n",
      "Train epoch: 18 [7680/60000(13%)]\t Loss: 56.260323\n",
      "Train epoch: 18 [8960/60000(15%)]\t Loss: 60.627621\n",
      "Train epoch: 18 [10240/60000(17%)]\t Loss: 59.046780\n",
      "Train epoch: 18 [11520/60000(19%)]\t Loss: 56.673477\n",
      "Train epoch: 18 [12800/60000(21%)]\t Loss: 58.472839\n",
      "Train epoch: 18 [14080/60000(23%)]\t Loss: 58.701145\n",
      "Train epoch: 18 [15360/60000(26%)]\t Loss: 58.079849\n",
      "Train epoch: 18 [16640/60000(28%)]\t Loss: 58.023445\n",
      "Train epoch: 18 [17920/60000(30%)]\t Loss: 57.220268\n",
      "Train epoch: 18 [19200/60000(32%)]\t Loss: 57.155838\n",
      "Train epoch: 18 [20480/60000(34%)]\t Loss: 58.301407\n",
      "Train epoch: 18 [21760/60000(36%)]\t Loss: 57.262291\n",
      "Train epoch: 18 [23040/60000(38%)]\t Loss: 59.603382\n",
      "Train epoch: 18 [24320/60000(41%)]\t Loss: 56.395428\n",
      "Train epoch: 18 [25600/60000(43%)]\t Loss: 60.664467\n",
      "Train epoch: 18 [26880/60000(45%)]\t Loss: 56.732143\n",
      "Train epoch: 18 [28160/60000(47%)]\t Loss: 56.784763\n",
      "Train epoch: 18 [29440/60000(49%)]\t Loss: 57.148205\n",
      "Train epoch: 18 [30720/60000(51%)]\t Loss: 58.967022\n",
      "Train epoch: 18 [32000/60000(53%)]\t Loss: 56.939075\n",
      "Train epoch: 18 [33280/60000(55%)]\t Loss: 56.140446\n",
      "Train epoch: 18 [34560/60000(58%)]\t Loss: 57.401051\n",
      "Train epoch: 18 [35840/60000(60%)]\t Loss: 57.593040\n",
      "Train epoch: 18 [37120/60000(62%)]\t Loss: 59.972179\n",
      "Train epoch: 18 [38400/60000(64%)]\t Loss: 57.980831\n",
      "Train epoch: 18 [39680/60000(66%)]\t Loss: 56.324501\n",
      "Train epoch: 18 [40960/60000(68%)]\t Loss: 59.772869\n",
      "Train epoch: 18 [42240/60000(70%)]\t Loss: 56.006950\n",
      "Train epoch: 18 [43520/60000(72%)]\t Loss: 56.404392\n",
      "Train epoch: 18 [44800/60000(75%)]\t Loss: 58.280346\n",
      "Train epoch: 18 [46080/60000(77%)]\t Loss: 56.638866\n",
      "Train epoch: 18 [47360/60000(79%)]\t Loss: 57.640099\n",
      "Train epoch: 18 [48640/60000(81%)]\t Loss: 57.607567\n",
      "Train epoch: 18 [49920/60000(83%)]\t Loss: 57.704586\n",
      "Train epoch: 18 [51200/60000(85%)]\t Loss: 56.782951\n",
      "Train epoch: 18 [52480/60000(87%)]\t Loss: 57.322540\n",
      "Train epoch: 18 [53760/60000(90%)]\t Loss: 56.044582\n",
      "Train epoch: 18 [55040/60000(92%)]\t Loss: 54.848972\n",
      "Train epoch: 18 [56320/60000(94%)]\t Loss: 57.706348\n",
      "Train epoch: 18 [57600/60000(96%)]\t Loss: 57.193657\n",
      "Train epoch: 18 [58880/60000(98%)]\t Loss: 57.168884\n",
      "====> Epoch: 18 Average loss: 57.6652\n"
     ]
    }
   ],
   "source": [
    "mse_loss = nn.BCELoss(size_average = False)\n",
    "\n",
    "\n",
    "def loss_function(W, x, recons_x, h, lam):\n",
    "    \"\"\"Compute the Contractive AutoEncoder Loss\n",
    "    See reference below for an in-depth discussion:\n",
    "      #1: https://contractiveae.blogspot.com/2020/03/hands-on-contractive-autoencoders.html\n",
    "    Args:\n",
    "        `W` (FloatTensor): (N_hidden x N), where N_hidden and N are the\n",
    "          dimensions of the hidden units and input respectively.\n",
    "        `x` (Variable): the input to the network, with dims (N_batch x N)\n",
    "        recons_x (Variable): the reconstruction of the input, with dims\n",
    "          N_batch x N.\n",
    "        `h` (Variable): the hidden units of the network, with dims\n",
    "          batch_size x N_hidden\n",
    "        `lam` (float): the weight given to the jacobian regulariser term\n",
    "    Returns:\n",
    "        Variable: the (scalar) CAE loss\n",
    "    \"\"\"\n",
    "    mse = mse_loss(recons_x, x)\n",
    "    # Since: W is shape of N_hidden x N. So, we do not need to transpose it as\n",
    "    # opposed to #1\n",
    "    dh = h * (1 - h) # Hadamard product produces size N_batch x N_hidden\n",
    "    # Sum through the input dimension to improve efficiency, as suggested in #1\n",
    "    w_sum = torch.sum(Variable(W)**2, dim=1)\n",
    "    # unsqueeze to avoid issues with torch.mv\n",
    "    w_sum = w_sum.unsqueeze(1) # shape N_hidden x 1\n",
    "    contractive_loss = torch.sum(torch.mm(dh**2, w_sum), 0)\n",
    "    return mse + contractive_loss.mul_(lam)\n",
    "\n",
    "\n",
    "model = CAE()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
    "\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for idx, (data, _) in enumerate(train_loader):\n",
    "        data = Variable(data)\n",
    "        if args.cuda:\n",
    "            data = data.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        hidden_representation, recons_x = model(data)\n",
    "\n",
    "        # Get the weights\n",
    "        # model.state_dict().keys()\n",
    "        # change the key by seeing the keys manually.\n",
    "        # (In future I will try to make it automatic)\n",
    "        W = model.state_dict()['fc1.weight']\n",
    "        loss = loss_function(W, data.view(-1, 784), recons_x,\n",
    "                             hidden_representation, lam)\n",
    "\n",
    "        loss.backward()\n",
    "        train_loss += loss.data[0]\n",
    "        optimizer.step()\n",
    "\n",
    "        if idx % args.log_interval == 0:\n",
    "            print('Train epoch: {} [{}/{}({:.0f}%)]\\t Loss: {:.6f}'.format(\n",
    "                  epoch, idx*len(data), len(train_loader.dataset),\n",
    "                  100*idx/len(train_loader),\n",
    "                  loss.data[0]/len(data)))\n",
    "\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "         epoch, train_loss / len(train_loader.dataset)))\n",
    "    model.samples_write(data,epoch)\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
